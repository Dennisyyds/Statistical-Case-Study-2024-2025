Statistical Case Study 2024-2025: ChatGPT vs. Human Essay Classification

This repository contains the files and scripts for a comprehensive statistical case study exploring the classification of ChatGPT-generated essays versus human-authored essays. The project evaluates the performance of various machine learning classifiers and the impact of data characteristics, parameters, and features on their effectiveness.

#Repository Structure

1. Folders

Code/
Contains R scripts for data preprocessing, feature extraction, classifier training, and evaluation.

Result-Study-6/
Stores results from Study 6, which focused on the impact of training data specificity on classifier performance.

2. Files

Figure.zip
Compressed folder containing visualizations, such as charts and graphs, that illustrate key findings from the study.

GPTessays.zip
A collection of raw essays generated by ChatGPT used as input data for analysis and classification.

humanessays.zip
A dataset of raw human-authored essays used for training and evaluation of classifiers.

functionwords.zip
Once you unzip it, there will be 2 main folders, “titles” and “functionwords”. 
The “functionwords” folder contains the function word counts for each of the human and GPTessays,
while the “titles” folder contains their titles.

output.zip
Compressed folder with output files, including all final results generated by the R scripts.

wordfile.txt
A text file listing the function words used in the analysis, which played a critical role in stylometric analysis and classification.

#Key Features of the Study

Classifier Evaluation
The study compares four machine learning classifiers: K-Nearest Neighbors (K-NN), Discriminant Analysis (DA), Random Forest (RF), and Support Vector Machines (SVMs). Their performance is evaluated based on accuracy, precision, recall, and F1 score.

Focus Areas
The analysis covers several key areas:
Impact of training data specificity.
Effect of essay length on classifier performance.
Importance of function word selection.
Role of training data size in generalization and accuracy.
Parameter optimization for K-NN and SVMs.

#How to Use This Repository

Data Preparation
Download and extract the functionwords.zip files into a designated directory.
Use the scripts in the Code/ folder to preprocess the data and extract features.

Analysis
Start by running Set Up.R to load required packages and initialize the environment.
Depending on your objective, run the relevant scripts:
For visualization, use Visualization.R
For study 1, use form_of_data.R
For study 2, use Different k for KNN.R or Test parameter of SVM.R
For study 4, use Determine Function Words.R
For study 5, use traindata_size.R
For study 6, use Except topic.R, Except_only_topic.R and OnlyTopic.R

The findings from this project lay the foundation for further research in authorship attribution. Future work could include:

Analyzing mixed-authorship essays (ChatGPT and human collaboration).
Expanding the dataset to include other generative models.
Developing hybrid or ensemble classifiers for improved accuracy across diverse scenarios.

#Contact

For questions or contributions, please reach out to Rongzhi Chen via S2148532@ed.ac.uk
